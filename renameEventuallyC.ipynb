{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JDS289/DNNs/blob/main/renameEventuallyC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BP-Q4keXyPAw"
      },
      "source": [
        "# Deep Neural Networks - First Assignment 2025\n",
        "## Ferenc Huszár and Nic Lane\n",
        "### Due date: 21 February, 2025\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Info"
      ],
      "metadata": {
        "id": "DeEax14-4ret"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Welcome to the first assignment for the Deep Learning and Neural Networks (DeepNN) module. This first half is marked out of 30, and makes up 30% of the total marks you can earn in this module. The second assignment is going to be a bit more substantial and more open-ended.\n",
        "\n",
        "\n",
        "There are 3 parts to this assignment, A, B and C, each worth 10% of the total module marks. The first two exercises contain specific subtasks, while the final one is more open-ended, requires you to do a bit of independent reading and experimentation.\n",
        "\n",
        "How to submit:\n",
        "* Please run this notebook in [google colab](https://colab.research.google.com/), adding your own code and text cells as requested\n",
        "* Leave relevant output (plots, etc) in the colab - assume we won't be running your code, but we want to check how you solved problems. Please name and submit the `ipynb` file.\n",
        "* For each of the three sections (A, B and C), please submit a 1 page pdf with:\n",
        "   * Up to 350 words of text summarising and interpreting of your findings.\n",
        "   * Up to 2 figures (or tables)\n",
        "   * For mathematical derivations, for which an extra appendix page is allowed. It's fine to include a screenshot of compiled latex or photo of hand written notes - if that saves you time.\n",
        "* For Sections A and B we included a writeup checklist to help you remember to include important components.\n",
        "* You do not have to max out the word count of figure numbers, in fact we prefer short, to the point, descriptions.\n",
        "* A correct solution of what we asked for with a solid writeup will earn full marks. Going beyond the brief - great, do it if you're curious - won't earn extra marks.\n",
        "\n",
        "Part of the goal of this assignment is for you to familiarise yourself with `pytorch`, if you don't know it already. To this end we tried to include quite a bit of explanation, skeleton code, and some hints."
      ],
      "metadata": {
        "id": "_XC8hHuX4mWD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "48tUJjZY0M8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "Y0L-Gf8g0OT0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67045b73-1f9e-4c96-a4fe-85d8800c6b65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#C: Inductive Bias of ReLU networks\n"
      ],
      "metadata": {
        "id": "pkwEStOO6BZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this exercise we ask you to run your own experiment that demonstrates/reproduces some known inductive biases of neural networks.\n",
        "\n",
        "Examples are:\n",
        "* **bias towards low-complexity boolean functions** As first demonstrated by [Vallé-Perez et al, 2018](https://arxiv.org/abs/1805.08522) randomly initialised neural networks (whose inputs and outputs are binary) have a tendency to implement lower complexity Boolean functions. **Hint:** Instead of using Lempel-Ziv complexity like in the paper, you may find it easier and/or more interesting to choose other measures of complexity, such as those mentioned in [this video](https://www.youtube.com/watch?v=XAhvVn1seUY).\n",
        "* **bias towards low-rank representations:** Look at, for example, Figures 1 or 2 of ([Huh et al, 2023](https://openreview.net/pdf?id=bCiNWDmlY2)). **Hint:** Training results from Figure 1 may be difficult to produce as you want to make sure you tune the learning rates for all depth/rank combinations. The rank analysis at initialization found in Figure 2. would take significantly less time to run.\n",
        "* **spline behaviour:** It has been shown ([Williams et al. 2020](https://proceedings.neurips.cc/paper_files/paper/2019/file/1f6419b1cbe79c71410cb320fc094775-Paper.pdf)) that - subject to some assumtions - gradient descent training dynamics in shallow ReLU networks leads to either cubic splines (in the so called kernel regime) or linear splines (in the so called adaptive regime). The difference between the two regimes is the scale of the random initialization. for details. **Hint:** It is not expected that you follow any details of the theoretical arguments in the paper linked above. It is fine for your investigation to be purely empirical: can you find an example where a trained ReLU network closely matches a cubic spline? Can you change the behaviour by changing initialization scale or learning rates?"
      ],
      "metadata": {
        "id": "dVkC1PSmwmnt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'd like to see if a probabalistic classifier will tend to learn generalisable probabbility reasoning, or whether this struggles to generalise beyond its training data. So for example, will a neural network end up implementing Bayes' Rule, or will it have a tendency towards \"if I see the x from training then output y, but if this, then that... and if it wasn't in training, then give a random output\"?"
      ],
      "metadata": {
        "id": "54txB2MYUVzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "IvuPHRygHYqD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distributions.categorical import Categorical\n",
        "\n",
        "keyboard = np.array([list(\"qwertyuiop\"), list(\"asdfghjkl;\"), list(\"zxcvbnm,./\")])\n",
        "flat_keyboard = keyboard.flatten()\n",
        "\n",
        "def distance(char1, char2):\n",
        "    return np.linalg.norm(np.concatenate(np.where(keyboard==char1)) - np.concatenate(np.where(keyboard==char2)))\n",
        "\n",
        "def prob_typo(char1, char2):  # unscaled logits\n",
        "    return 7 / (1+distance(char1, char2))"
      ],
      "metadata": {
        "id": "QOwqMbKUm2YZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# variable names ending in \"_i\" mean that letters are represented by their position in flat_keyboard, rather than as a char\n",
        "\n",
        "word_list = np.array([\"past\", \"last\", \"part\", \"pest\", \"lest\", \"kart\"])\n",
        "word_probs = torch.tensor([0.25, 0.25, 0.25, 0.15, 0.05, 0.05])\n",
        "char_to_index = {letter: idx for idx, letter in enumerate(flat_keyboard)}\n",
        "word_list_i = torch.tensor([[char_to_index[char] for char in word] for word in word_list])\n",
        "intended_words_choices = Categorical(probs=word_probs).sample((10**6,))\n",
        "intended_words, intended_words_i = word_list[intended_words_choices], word_list_i[intended_words_choices]\n",
        "char_typo_logits = torch.stack([torch.tensor([prob_typo(intended, typo) for typo in flat_keyboard]) for intended in flat_keyboard])\n",
        "\n",
        "#sample_i = Categorical(logits=char_typo_logits[intended_words_i]).sample()\n",
        "#typod_words = [\"\".join(letters) for letters in flat_keyboard[sample_i]]\n",
        "#sample = (typod_words, intended_words)"
      ],
      "metadata": {
        "id": "CZRH5ZFS7vCM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(\"drive/MyDrive/DNNs1/sample.pkl\", \"wb\") as f: pickle.dump(sample, f)"
      ],
      "metadata": {
        "id": "kA5AJqHyh30G"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_words(words):\n",
        "    def encode(word):\n",
        "        if word not in encode_words.word_cache:\n",
        "            encode_words.word_cache[word] = np.concatenate([(flat_keyboard==letter).astype(float) for letter in word])\n",
        "        return encode_words.word_cache[word]\n",
        "    return np.array([encode(word) for word in words])\n",
        "encode_words.word_cache = {}\n",
        "\n",
        "def sample_to_inputs(typod_words):\n",
        "    return torch.tensor(encode_words(typod_words), device=DEVICE)\n",
        "\n",
        "def sample_to_indices(intended_words):  # \"indices\" are the class labels\n",
        "    mapping = {word: idx for idx, word in enumerate(word_list)}\n",
        "    return torch.tensor([mapping[word] for word in intended_words], device=DEVICE)"
      ],
      "metadata": {
        "id": "LYZlEH1q1Sdz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import Sequential, Linear, ReLU, LogSoftmax, NLLLoss\n",
        "\n",
        "def middle_layer():\n",
        "    layer = Linear(120, 120)\n",
        "    return layer\n",
        "\n",
        "def output_layer():\n",
        "    layer = Linear(120, 6)\n",
        "    return layer\n",
        "\n",
        "def get_typo_network(num_hidden_layers=10):\n",
        "    if num_hidden_layers <= 0:\n",
        "        raise ValueError('Number of hidden layers must be positive')\n",
        "    blocks = []\n",
        "    for l in range(num_hidden_layers):\n",
        "        blocks.append(middle_layer())\n",
        "        blocks.append(ReLU())\n",
        "    blocks.append(output_layer())\n",
        "    blocks.append(LogSoftmax(dim=1))\n",
        "    return Sequential(*blocks)"
      ],
      "metadata": {
        "id": "fNuzjfFXnlz5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(DEVICE)\n",
        "\n",
        "def gradient_steps_with_split(model, xs_train, ys_train, xs_test, ys_test, lr, iterations, optimizer=torch.optim.Adam):\n",
        "    \"\"\"Performs the specified amount of Adam optimization steps on a Typo model using the training set,\n",
        "       but printing loss for a separate test set.\"\"\"\n",
        "    optimizer = optimizer(model.parameters(), lr=lr)\n",
        "    loss_fn = NLLLoss()\n",
        "\n",
        "    for i in range(iterations):\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(model(xs_train.float()), ys_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if i%100 == 0:\n",
        "            print(loss_fn(model(xs_test.float()), ys_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6pVnDOxqlef",
        "outputId": "d4537bb6-386d-468e-b6d6-168acd5bcb50"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "model = get_typo_network()\n",
        "model.to(DEVICE)\n",
        "split = len(sample[0])*4 // 5\n",
        "inputs_train = sample_to_inputs(sample[0][:split])\n",
        "indices_train = sample_to_indices(sample[1][:split])\n",
        "inputs_test = sample_to_inputs(sample[0][split:])\n",
        "indices_test = sample_to_indices(sample[1][split:])\n",
        "gradient_steps_with_split(model, inputs_train, indices_train, inputs_test, indices_test, 0.0001, 3000)\n",
        "\"\"\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "jit4p9jmypuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "model = get_typo_network()\n",
        "model.load_state_dict(torch.load(\"drive/MyDrive/DNNs1/parameters.pth\", weights_only=True, map_location=DEVICE))\n",
        "model.to(DEVICE);\n",
        "\n",
        "with open(\"drive/MyDrive/DNNs1/sample.pkl\", \"rb\") as f:  sample = pickle.load(f)"
      ],
      "metadata": {
        "id": "w_3khQElbVzN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "524e0af6-49dc-41a2-e32b-cbb7865621a0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction(model, input_word):\n",
        "    return \"\\n\".join([f\"{word}: {round(prob.item()*100, 2)}%\" for word, prob in zip(word_list, torch.exp(model(sample_to_inputs([input_word]).float())[0]))])\n",
        "\n",
        "print(prediction(model, \"ldst\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHphnCJQcVtD",
        "outputId": "0e3edccd-69d4-4bbd-ebd2-0f810f7beeed",
        "collapsed": true
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "past: 1.02%\n",
            "last: 61.17%\n",
            "part: 0.07%\n",
            "pest: 2.02%\n",
            "lest: 35.67%\n",
            "kart: 0.05%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_logits = Categorical(logits=torch.tensor([prob_typo(\"e\", typo) for typo in flat_keyboard])).logits\n",
        "print(\"\\n\".join(map(lambda t: f\"{t[0]}: {round(torch.sigmoid(t[1]).item(), 3)}\", sorted(zip(flat_keyboard, normalized_logits), key=lambda t:-t[1])[:10])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFs5Y1VHndFS",
        "outputId": "64257195-9635-4bb4-d728-cc2854ab230d",
        "collapsed": true
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e: 0.446\n",
            "w: 0.024\n",
            "r: 0.024\n",
            "d: 0.024\n",
            "s: 0.013\n",
            "f: 0.013\n",
            "q: 0.008\n",
            "t: 0.008\n",
            "c: 0.008\n",
            "a: 0.006\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_logits = Categorical(logits=torch.tensor([prob_typo(\"a\", typo) for typo in flat_keyboard])).logits\n",
        "print(\"\\n\".join(map(lambda t: f\"{t[0]}: {round(torch.sigmoid(t[1]).item(), 3)}\", sorted(zip(flat_keyboard, normalized_logits), key=lambda t:-t[1])[:13])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6hSmaHNvysd",
        "outputId": "4a1522d1-6af8-4ff9-a248-5e5953e0b7ca",
        "collapsed": true
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a: 0.453\n",
            "q: 0.024\n",
            "s: 0.024\n",
            "z: 0.024\n",
            "w: 0.014\n",
            "x: 0.014\n",
            "d: 0.008\n",
            "e: 0.007\n",
            "c: 0.007\n",
            "f: 0.004\n",
            "r: 0.004\n",
            "v: 0.004\n",
            "g: 0.003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(prediction(model, \"les[\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0e7cxZfnFt_",
        "outputId": "771f02b5-8c8b-4a13-e309-419c1e6e99a9",
        "collapsed": true
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "past: 0.21%\n",
            "last: 7.99%\n",
            "part: 0.02%\n",
            "pest: 5.62%\n",
            "lest: 86.16%\n",
            "kart: 0.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Single-quotes means what was intended; double-quotes means what was typed:\n",
        "\n"
      ],
      "metadata": {
        "id": "tLAENiEPrDhC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\text{Bayesianism using the actual distributions: }~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\\\ \\frac{P(`\\text{lest'} ~ | ~ ``\\text{les[\"})}{P(`\\text{last'} ~|~ ``\\text{les[\"})} = \\frac{P(``\\text{les[\"} ~ | ~ `\\text{lest'}) ~ P(`\\text{lest'}) / P(``\\text{les[\"})} {P(``\\text{les[\"} ~ | ~ `\\text{last'}) ~ P(`\\text{last'}) / P(``\\text{les[\"})} \\\\[0.3in] =\n",
        "0.2 ~ \\frac{{P(``\\text{les[\"} ~ | ~ `\\text{lest'}})} {{P(``\\text{les[\"} ~ | ~ `\\text{last'}})} = 0.2 ~ \\frac{P(``\\text{e\"} ~|~ `\\text{e'}) }{P(``\\text{e\"} ~|~ `\\text{a'}) }~\\approx~ 0.2 ~  \\frac{0.446}{0.007} ~~\\approx~ 12.7\n",
        "\\\\[0.5in]\n",
        "\\text{Model's result, having never seen this example: }~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\\\\n",
        "\\frac{86.16}{7.99} ~ \\approx ~ 10.8\n",
        "\\\\[0.6in]\n",
        "$$"
      ],
      "metadata": {
        "id": "5lM546uC0ZCI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is pretty close, and particularly impressive for an example it had never seen during training. In fact, something non-trivial that the model handled well: a typed \"r\" in second position is enough to overcome the prior of P('last') = 5 P('lest'), but a \"d\" in second position is not (that is, an \"r\" makes the posterior probability of 'lest' overtake 'last', whereas a \"d\" does not) -- this is because although 'e'→\"d\" is a 4× more likely typo than 'a'→\"d\", this is still less than the prior of 5×, whereas 'e'→\"r\" is **6**× more likely than 'a'→\"r\".\n",
        "\n",
        "\n",
        "So it seems like it has learned some generalisable notions about probability, rather than just memorising the training answers."
      ],
      "metadata": {
        "id": "Ummkw_Gn3h8f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another property we want -- a notion of \"invariance over irrelevant changes\", i.e. we don't want a change to the *final* letter to randomly change its preferences for \"past\" vs \"last\" (given that they only differ in the first letter, and my sample-generation assumed independence)."
      ],
      "metadata": {
        "id": "s6lvd7R-NnQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(prediction(model, \"pas.\"))\n",
        "print(\"\\n\")\n",
        "print(prediction(model, \"pas;\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5CZzHx9MRSz",
        "outputId": "2117ff26-6d7a-4561-90ad-2a08a80d7bc7",
        "collapsed": true
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "past: 96.59%\n",
            "last: 1.71%\n",
            "part: 1.01%\n",
            "pest: 0.68%\n",
            "lest: 0.0%\n",
            "kart: 0.0%\n",
            "\n",
            "\n",
            "past: 97.18%\n",
            "last: 1.8%\n",
            "part: 0.75%\n",
            "pest: 0.28%\n",
            "lest: 0.0%\n",
            "kart: 0.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is quite consistent -- the percentages are very similar, and the \"preference ranking\" stays identical. It's not perfect though, so the underlying function probably isn't as simple as we'd like."
      ],
      "metadata": {
        "id": "iAecKbOGOkub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section C Writeup Checklist\n",
        "\n",
        "This is an exploratory project, but here is an example of what components you want to include:\n",
        "* a hypothesis: what is the motivation for your work, what do you expect to see or demonstrate.\n",
        "* brief description of methodology, network/optimization parameters, etc\n",
        "* a figure or two summarising your findings\n",
        "* discussion of whether findings support your hypothesis or not, perhaps speculation or explanation about what is going on.\n",
        "\n",
        "Note: if you run your experiments and they don't produce the expected effect, that is perfectly fine, so long as your experiment made sense. Our existing understanding of inductive biases is far from complete, and the observations above may not universal for all architectures and training regimes."
      ],
      "metadata": {
        "id": "kke7CLZ-vE3v"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "DeEax14-4ret",
        "a1vgSdt8eDvK",
        "Xr4LdCiQfF9A",
        "RYBjnQak_24-",
        "cFNw05QOidgu",
        "mnz_7oloAA_t",
        "nuiAjQmRu6nV",
        "TLWuahzsAMvE",
        "-NY8lVwKFJvE",
        "GUi9QRhGAewP",
        "Qfy79eIrTFfW",
        "cLVCqhz5Ao8c",
        "9axs5GFUyIe3",
        "mucks6jd2k_n",
        "SiMUWjaaA6hR",
        "fg_CxU7ICFmf",
        "fFp5PouEUW9J"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}