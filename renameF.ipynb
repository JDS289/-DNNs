{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JDS289/-DNNs/blob/main/renameF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0284MVa_Tf4D"
      },
      "source": [
        "# Deep Learning and Neural Networks Second Assignment 2024\n",
        "### Ferenc Huszár and Nic Lane\n",
        "#### due date: Friday, 21 March 2025, 12:00 PM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koMibeX-o_Hm"
      },
      "source": [
        "##Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrN8SNDMjiYF"
      },
      "source": [
        "Welcome to the second assignment for the Deep Neural Networks module. In this assignment you will explore some of the model architectures we talked about in the second half of lectures (ConvNets, Transformers) and you will also implement a fun model called MLP Mixer.\n",
        "\n",
        "There are 70 marks given in total for this second assessment ($70\\%$ of the total of 100 marks for the course), broken into three sections:\n",
        "* (D) 10 marks for the ConvNet exercise\n",
        "* (E) 20 marks for the Transformer/MLP Mixer exercise\n",
        "* (F) 40 marks for a mini-project of your choice\n",
        "\n",
        "\n",
        "#### Mini-projects\n",
        "\n",
        "Mini-project tasks are a more exploratory and open-ended, giving you an opportunity to decide which aspect you'd like to focus on. The idea is to introduce you to the form of assess\\ment typical in our Part III/MPhil modules. Mini-projects come with instructions, to indicate the depth of work we expect for certain marks, but you should feel free to deviate from instructions if you have a better idea to explore within the context. There are two options, one with a more theory/maths focus, one with more of an engineering focus.\n",
        "\n",
        "As a guide, when marking we will take into account three factors:\n",
        "\n",
        "* **extent of work:** did you do the expected amount of work (you won't get extra marks by doing a lot more than others, this is not a race). We will try to give an indication of this in the module description.\n",
        "* **correctness/technical understanding:** is your solution and description of findings technincally correct, does it demonstrate learning and understanding of the topics we cover, and an ability to do independent reading if needed?\n",
        "* **presentation:** How is the mini-project written up? Please focus on the writeup being short, to the point, well structured. Are figures well formatted, so it's clear what's shown on them (e.g. are there axis labels)?\n",
        "\n",
        "You can choose whichever project you want to attempt. You can attempt more than one, but we will only mark one. **Please clearly state which of the mini-projects you would like us to mark**, if this is unclear, we will mark whichever appears fist in your submitted notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MnGtmMPz1FS"
      },
      "source": [
        "# F.1: Compositional Generalisation in Group Structured Data\n",
        "_[40 marks] 4-page writeup plus appendices_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vol2gk4_W_V5"
      },
      "source": [
        "The starting point for this exercise is Section E, where you trained models to predict missing entries of the Cayley table for $\\mathbb{Z}_{97}$ given only a random subset of entries. If your models worked well there, and you like groups, this mini-project might appeal to you.\n",
        "\n",
        "There is a lot of interesting compositional structure in groups. The 97-element cyclic group is rather boring, it is a simple group, meaning its only subgroups are the trivial group $\\{e\\}$ and itself. This is because $97$ is a prime number. Cyclic groups with non-prime size $n$, on the other hand, have more intereseting subgroup structure: For every divisor $d$ of $n$, there is exactly one subgroup of order (size) $d$. Symmetric groups (groups over permutations) have even more interesting subgroup structure. For example, $S_4$ contains 24 elements, and several subgoups of sizes corresponding to every divisor of 24. See details [on Wikipedia](https://en.wikipedia.org/wiki/Subgroup#Example:_Subgroups_of_S4). Elementary Albelian groups $\\mathbb{Z}_p^k$ are another example of groups with rich subgroup structure.\n",
        "\n",
        "In this mini-project, your task is to collect evidence relating to the following hypothesis:\n",
        "\n",
        "_Hypothesis:_ Transformers (and/or other models) are able to learn the full Cayley table of rich groups like $S_n$ or $\\mathbb{Z}_p^k$ seeing data only from certain sets of subgroups (i.e. specific slices of the Cayley table).\n",
        "\n",
        "We know that small transformers are able to learn the Cayley table of $S_5$ when a small (less than 50%) fraction of randomly selected entries is provided as training data. To test the hypothesis above, you would design a different training-test split strategy which selects the union of certain subgroups as training data, with the remaining entries as test data. Is sampling subgroups better or worse than random sampling?\n",
        "\n",
        "High marks for a complete investigation, including commentary about what your findings might reveal about compositional and statistical generalisation in Transformers and/or other models. It is expected that you put more effort into tuning your hyperparameters and improving your models than what is expected for part E alone."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAefsoFPswD4"
      },
      "source": [
        "##Groups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_yPlCO2LLMU8"
      },
      "outputs": [],
      "source": [
        "from typing import Iterable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dpRMvs6C_Dmc"
      },
      "outputs": [],
      "source": [
        "class Permutation(tuple):\n",
        "    def __getitem__(self, indices: int | slice | Iterable):\n",
        "        if type(indices) in (int, slice):\n",
        "            return tuple.__getitem__(self, indices)\n",
        "\n",
        "        assert isinstance(indices, Permutation)\n",
        "        return Permutation([self[i] for i in indices])\n",
        "\n",
        "    def __matmul__(self, other):  # f@g, composition\n",
        "        return self[other]\n",
        "\n",
        "    def __pow__(self, exponent):  # repeated composition\n",
        "        if exponent == 0:\n",
        "            return Permutation(range(len(self)))  # identity\n",
        "        return self @ self**(exponent-1)\n",
        "\n",
        "    def __invert__(self):  # equivalent to argsort\n",
        "        return Permutation([self.index(i) for i in range(len(self))])\n",
        "\n",
        "    def __call__(self, other: int | Iterable):\n",
        "        return self[other]\n",
        "\n",
        "    def insert(self, index, value: int):\n",
        "        assert value == len(self)\n",
        "        return Permutation(self[:index] + (value,) + self[index:])\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        return type(other)==type(self) and tuple.__eq__(self, other)\n",
        "    def __ne__(self, other):\n",
        "        return not self==other\n",
        "    def __hash__(self):\n",
        "        return hash((type(self), tuple.__hash__(self)))\n",
        "\n",
        "    def __repr__(self):\n",
        "        return tuple.__repr__(self).replace(\"(\", \"⟦\").replace(\",)\", \"⟧\").replace(\")\", \"⟧\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sN6HIlIz5IOY"
      },
      "outputs": [],
      "source": [
        "class Tuple(tuple):\n",
        "    def __new__(cls, iterable=None):\n",
        "        if iterable is None:\n",
        "            iterable = ()\n",
        "        return super().__new__(cls, tuple(item for item in iterable if item!=Tuple()))\n",
        "\n",
        "    def __invert__(self):\n",
        "        return Tuple(~item for item in self)\n",
        "\n",
        "    def __matmul__(self, other):\n",
        "        assert all(len(p1)==len(p2) for p1, p2 in zip(self, other))\n",
        "        return Tuple(p1@p2 for p1, p2 in zip(self, other))\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        return type(other)==type(self) and tuple.__eq__(self, other)\n",
        "    def __ne__(self, other):\n",
        "        return not self==other\n",
        "    def __hash__(self):\n",
        "        return hash((type(self), tuple.__hash__(self)))\n",
        "\n",
        "    def __repr__(self):\n",
        "        return tuple.__repr__(self).replace(\"(\", \"⦅\").replace(\",)\", \"⦆\").replace(\")\", \"⦆\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oR_9VTTqWzsj"
      },
      "outputs": [],
      "source": [
        "class Group(set):\n",
        "    def __init__(self, iterable: Iterable[Permutation | Tuple], check_valid=True):\n",
        "        assert len(iterable) >= 1\n",
        "        if len(iterable) == 1 and () in iterable:\n",
        "            iterable = [Tuple()]\n",
        "        super().__init__(set(iterable))\n",
        "\n",
        "        if check_valid==False or (len(iterable)==1 and Tuple() in iterable):\n",
        "            return\n",
        "\n",
        "        assert all(~obj in self for obj in self)  # inverses\n",
        "        assert all(objSecond@objFirst in self for objFirst, objSecond in self**2)  # closure;  given inverses, this implies an identity\n",
        "        assert all((objThird@objSecond)@objFirst == objThird@(objSecond@objFirst)  for objFirst, (objSecond, objThird) in self**3) # associativity\n",
        "\n",
        "\n",
        "    def __mul__(self, other):  # cartesian product\n",
        "        assert isinstance(other, Group)   # then the product is a valid group...\n",
        "        return Group({Tuple((obj1, obj2)) for obj1 in self for obj2 in other}, check_valid=False) # ...so we don't need to check again\n",
        "\n",
        "    def __iter__(self):\n",
        "        return set.__iter__(self)\n",
        "\n",
        "    def __pow__(self, exponent):\n",
        "        if exponent==1:\n",
        "            return self\n",
        "        if exponent == 0:\n",
        "            return Group({()})\n",
        "        return self * self**(exponent-1)\n",
        "\n",
        "    def __repr__(self):\n",
        "        if len(self) == 0:\n",
        "            return \"⦃ ⦄\"\n",
        "        return f\"⦃{repr(set(self))[1:-1]}⦄\"\n",
        "\n",
        "    @property\n",
        "    def table(self):\n",
        "        return {(obj2, obj1) : obj2@obj1  for obj1, obj2 in self**2}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sBljcVQGI5GT"
      },
      "outputs": [],
      "source": [
        "def S(n):  # the Symmetric Group (i.e. the set of permutations) of {0, ..., n-1}\n",
        "    if n == 0:\n",
        "        return Group({Permutation([])})\n",
        "    return Group({perm.insert(index, n-1)  for perm in S(n-1) for index in range(n)}, check_valid=False)  # we know S_n is a valid group!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GA9N9ADs0Wv"
      },
      "source": [
        "##Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1HJkArEY3lOK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, Dataset, DataLoader, ChainDataset, random_split\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import random\n",
        "from math import factorial\n",
        "from google.colab import drive\n",
        "\n",
        "def random_batch_split(num_batches, train): # random_split can take [0.1]*10, but seems to fail for [0.05]*20, so here's a workaround\n",
        "    if num_batches <= 10:\n",
        "        batch_list = num_batches * [1/num_batches]\n",
        "    else:\n",
        "        batch_list = num_batches * [len(train)//num_batches]\n",
        "        num_remaining = len(train) - sum(batch_list)\n",
        "        for i in range(num_remaining):\n",
        "            batch_list[i] += 1\n",
        "    return random_split(train, batch_list, generator=torch.Generator().manual_seed(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kqCYc08qsm5C"
      },
      "outputs": [],
      "source": [
        "class EncodedSymmetricGroupTransformer(nn.Module): # with each permutation represented by a single integer\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        transformer_layer = nn.TransformerEncoderLayer(embed_dim, num_heads, dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers, enable_nested_tensor=False)\n",
        "        self.linear = nn.Linear(embed_dim, vocab_size)\n",
        "        self.pos_embedding = nn.Parameter(torch.zeros(2, embed_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedding = self.embedding(x)\n",
        "        token_emb = embedding + self.pos_embedding[:embedding.size(1), :]\n",
        "        encoded = self.transformer_encoder(token_emb.transpose(0, 1))\n",
        "        return self.linear(encoded.mean(dim=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "eqDnJz6TsrnY"
      },
      "outputs": [],
      "source": [
        "def format_losses(epoch, training_loss, validation_loss):\n",
        "    return f\"   ({epoch}, {training_loss:.4f}, {validation_loss:.4f})\"\n",
        "\n",
        "\n",
        "def train_model(model, train, validation, num_epochs, learning_rate, num_batches=5, weight_decay=0.05, betas=(0.95, 0.999)):\n",
        "    validation_inputs, validation_labels = validation[::]\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay, betas=betas)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    def print_losses(model, epoch):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            avg_training_loss, validation_loss = (loss_fn(model(train[::][0][:50000]), train[::][1][:50000]),\n",
        "                                                  loss_fn(model(validation_inputs[:50000]), validation_labels[:50000]))\n",
        "        print(format_losses(epoch, avg_training_loss, validation_loss), end=\"\")\n",
        "        model.train()\n",
        "\n",
        "    print(\"(Epoch, Training Loss, Validation Loss):\")\n",
        "    print_losses(model, 0)\n",
        "\n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        optimizer.zero_grad()\n",
        "        for batch in random_batch_split(num_batches, train):\n",
        "            input_batch, label_batch = batch[::]\n",
        "            training_loss = loss_fn(model(input_batch), label_batch)\n",
        "            training_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if 5*epoch//num_epochs > 5*(epoch-1)//num_epochs:\n",
        "            print_losses(model, epoch)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        probs = F.softmax(model(validation_inputs[:50000]), dim=-1)\n",
        "        predicted = probs.argmax(dim=-1)\n",
        "        accuracy = (predicted==validation_labels[:50000]).float().mean().item()\n",
        "        print('\\n    Accuracy of \"best-guess\"es on the validation set:\"', f\"{accuracy*100:.2f}%\", \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "utBe62RM4MlA"
      },
      "outputs": [],
      "source": [
        "class SymmetricGroupDataset(TensorDataset):\n",
        "    def __init__(self, base_set_size, alphabet=None):  # `base_set_size` i.e. the `n` in S_n\n",
        "        if alphabet is None:\n",
        "            alphabet = list(range(factorial(base_set_size)))\n",
        "        random.seed(10)\n",
        "        random.shuffle(alphabet)\n",
        "        self.alphabet = alphabet\n",
        "        self.group = S(base_set_size)\n",
        "\n",
        "        self.mapping_dict = {}\n",
        "        self.mapping_inverse_dict = {}\n",
        "        for permutation, token in zip(self.group, self.alphabet):\n",
        "            self.mapping_dict[token] = permutation\n",
        "            self.mapping_inverse_dict[permutation] = token\n",
        "\n",
        "        inputs = []\n",
        "        outputs = []\n",
        "        for (perm1, perm2), perm3 in self.group.table.items():\n",
        "            inputs.append([self.unmap(perm1), self.unmap(perm2)])\n",
        "            outputs.append(self.unmap(perm3))\n",
        "\n",
        "        self.tensors = (torch.tensor(inputs), torch.tensor(outputs))   # self.tensors is used by parent class\n",
        "\n",
        "    def map(self, token):\n",
        "        return self.mapping_dict[token]\n",
        "\n",
        "    def unmap(self, permutation):\n",
        "        return self.mapping_inverse_dict[permutation]\n",
        "\n",
        "    def cuda(self):\n",
        "        self.tensors = (self.tensors[0].cuda(), self.tensors[1].cuda())\n",
        "        return self"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QmbKkq2j0kzX"
      },
      "outputs": [],
      "source": [
        "subgroups_s5 = [set(), set(), set(), set(), set(), set()]\n",
        "\n",
        "for perm in S(6):\n",
        "    for i, element in enumerate(perm):\n",
        "        if i==element:\n",
        "            subgroups_s5[i].add(perm)\n",
        "\n",
        "derangements = set(S(6)) - set().union(*subgroups_s5)\n",
        "subgroups_s5 = [Group(s, check_valid=False) for s in subgroups_s5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "wMwZlAg78hv2"
      },
      "outputs": [],
      "source": [
        "base_set_size = 6\n",
        "whole_dataset = SymmetricGroupDataset(base_set_size)\n",
        "u = whole_dataset.unmap\n",
        "\n",
        "training_set, validation_set, holdout_set = random_split([(i, j) for i in range(720) for j in range(720)],\n",
        "                                                [0.5, 0.1, 0.4], generator=torch.Generator().manual_seed(10))\n",
        "training_set = set(training_set)\n",
        "validation_set = set(validation_set)\n",
        "holdout_set = set(holdout_set)\n",
        "# note that for now, training and validation only happen on the intersection with subgroups_s5,\n",
        "# so in reality the holdout set is actually much bigger\n",
        "\n",
        "subgroupsTrain = []\n",
        "subgroupsValidation = []\n",
        "\n",
        "for i in range(6):\n",
        "    train_inputs = []\n",
        "    train_labels = []\n",
        "    validation_inputs = []\n",
        "    validation_labels = []\n",
        "    table = list(subgroups_s5[i].table.items())\n",
        "    random.shuffle(table)\n",
        "    for (p1, p2), p3 in table:\n",
        "        if (u(p1), u(p2)) in training_set:\n",
        "            train_inputs.append((u(p1), u(p2)))\n",
        "            train_labels.append(u(p3))\n",
        "        elif (u(p1), u(p2)) in validation_set:\n",
        "            validation_inputs.append((u(p1), u(p2)))\n",
        "            validation_labels.append(u(p3))\n",
        "\n",
        "    subgroupsTrain.append(TensorDataset(torch.tensor(train_inputs).cuda(), torch.tensor(train_labels).cuda()))\n",
        "    subgroupsValidation.append(TensorDataset(torch.tensor(validation_inputs).cuda(), torch.tensor(validation_labels).cuda()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "1eDXK0okEUFe"
      },
      "outputs": [],
      "source": [
        "model = EncodedSymmetricGroupTransformer(vocab_size=factorial(base_set_size), embed_dim=64, num_heads=4, num_layers=3, dropout=0.2).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcSnOW0z6Tan",
        "outputId": "80dc3c6f-ffbb-40b1-a4fd-8124f3779bfb",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch, Training Loss, Validation Loss):\n",
            "   (0, 6.7095, 6.7087)   (200, 3.9393, 5.4489)   (400, 0.3771, 7.0608)   (600, 0.0561, 4.3936)   (800, 0.0313, 0.1549)   (1000, 0.0434, 0.0524)\n",
            "    Accuracy of \"best-guess\"es on the validation set:\" 100.00% \n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_model(model, subgroupsTrain[0], subgroupsValidation[0], num_epochs=1000, learning_rate=0.003, num_batches=1, weight_decay=1.5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.save(model.state_dict(), \"/content/drive/MyDrive/DNNs2/earlyStage_state_dict.pth\")"
      ],
      "metadata": {
        "id": "RHK1nUxrQg7Y"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/DNNs2/earlyStage_state_dict.pth\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-1j65CLQ2NE",
        "outputId": "acacdc4d-4e97-4d6f-ff9d-6fcb4b964b48"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WS3IZP6Cb7SS",
        "outputId": "1520329d-f0aa-4abe-e0e2-f0712d5f6073",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch, Training Loss, Validation Loss):\n",
            "   (0, 8.8561, 8.8484)   (100, 4.6541, 4.7725)   (200, 3.6789, 3.8957)   (300, 3.0644, 3.1324)   (400, 1.9544, 2.0055)   (500, 0.6074, 0.6373)\n",
            "    Accuracy of \"best-guess\"es on the validation set:\" 99.32% \n",
            "\n",
            "(Epoch, Training Loss, Validation Loss):\n",
            "   (0, 10.1599, 10.1220)   (100, 4.4720, 4.5686)   (200, 3.3447, 3.6584)   (300, 2.5849, 2.9697)   (400, 1.4929, 1.6129)   (500, 0.4766, 0.5305)\n",
            "    Accuracy of \"best-guess\"es on the validation set:\" 99.51% \n",
            "\n",
            "(Epoch, Training Loss, Validation Loss):\n",
            "   (0, 10.4990, 10.5969)   (100, 4.3033, 4.4606)   (200, 2.8723, 3.2337)   (300, 1.5258, 1.5947)   (400, 0.6681, 0.7048)   (500, 0.2184, 0.2413)\n",
            "    Accuracy of \"best-guess\"es on the validation set:\" 99.93% \n",
            "\n",
            "(Epoch, Training Loss, Validation Loss):\n",
            "   (0, 10.4515, 10.4012)   (100, 3.1577, 3.3768)   (200, 1.4835, 1.5819)   (300, 0.5806, 0.6129)   (400, 0.1665, 0.1850)   (500, 0.0520, 0.0631)\n",
            "    Accuracy of \"best-guess\"es on the validation set:\" 100.00% \n",
            "\n",
            "(Epoch, Training Loss, Validation Loss):\n",
            "   (0, 9.7772, 9.7807)   (100, 3.0939, 3.3069)   (200, 1.1797, 1.2484)   (300, 0.3783, 0.4081)   (400, 0.1077, 0.1200)   (500, 0.0375, 0.0435)\n",
            "    Accuracy of \"best-guess\"es on the validation set:\" 100.00% \n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(1, 6):\n",
        "    train_model(model, subgroupsTrain[i], subgroupsValidation[i], num_epochs=500, learning_rate=0.0006, num_batches=1, weight_decay=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "cW17q-lBhFCi"
      },
      "outputs": [],
      "source": [
        "def combineDatasets(datasets):  # because I can't get Torch's version(s) to work...\n",
        "    return TensorDataset(torch.cat([d.tensors[0] for d in datasets]), torch.cat([d.tensors[1] for d in datasets]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.save(model.state_dict(), \"/content/drive/MyDrive/DNNs2/midStage_state_dict.pth\")"
      ],
      "metadata": {
        "id": "gboEtVLFSIp5"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/DNNs2/midStage_state_dict.pth\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CQrqneBSUH4",
        "outputId": "1799fcb6-69bf-4e4a-dc83-395af1b7cecd"
      },
      "execution_count": 341,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 341
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 342,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYdunClPeHCQ",
        "outputId": "dc8e25ea-9f1f-47bf-8869-61969472d977"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch, Training Loss, Validation Loss):\n",
            "   (0, 6.7663, 6.7231)   (60, 1.7538, 1.8973)   (120, 0.5255, 0.7075)   (180, 0.1022, 0.1904)   (240, 0.0190, 0.0401)   (300, 0.0062, 0.0137)\n",
            "    Accuracy of \"best-guess\"es on the validation set:\" 99.99% \n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_model(model, combineDatasets(subgroupsTrain), combineDatasets(subgroupsValidation), num_epochs=300,\n",
        "            learning_rate=0.002, num_batches=1, weight_decay=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.save(model.state_dict(), \"/content/drive/MyDrive/DNNs2/midLateStage_state_dict.pth\")"
      ],
      "metadata": {
        "id": "hd_3PR3fSx0C"
      },
      "execution_count": 346,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table = S(6).table\n",
        "unseen_inputs = []\n",
        "unseen_labels = []\n",
        "\n",
        "for (u_p1, u_p2), u_p3 in set(map(lambda t: ((u(t[0][0]), u(t[0][1])), u(t[1])), table.items()))   \\\n",
        "                       - (set(zip(train_inputs, train_labels)) | set(zip(validation_inputs, validation_labels))):\n",
        "    unseen_inputs.append((u_p1, u_p2))\n",
        "    unseen_labels.append(u_p3)"
      ],
      "metadata": {
        "id": "6_0D7a5JVHzc"
      },
      "execution_count": 345,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unseen = TensorDataset(torch.tensor(unseen_inputs).cuda(), torch.tensor(unseen_labels).cuda())\n",
        "train, validation, holdout = random_split(unseen, [0.035, 0.005, 0.96])"
      ],
      "metadata": {
        "id": "w-hTeiF7_Kjw"
      },
      "execution_count": 360,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/DNNs2/midLateStage_state_dict.pth\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geeMffY4c2AE",
        "outputId": "6e61fad2-d68f-4fe6-946f-3ff52f7ef7db"
      },
      "execution_count": 396,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 396
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, train, validation, num_epochs=1500, learning_rate=0.002, num_batches=1, weight_decay=0.0001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KXwpL1K_LJk",
        "outputId": "e8b04611-9ad2-47ad-84d2-f74efd335d98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch, Training Loss, Validation Loss):\n",
            "   (0, 9.3693, 9.4054)   (300, 0.0065, 0.0445)   (600, 0.0004, 0.0047)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import pickle\n",
        "#with open(\"/content/drive/MyDrive/DNNs2/unmap.pkl\", \"wb\") as f:\n",
        "#    pickle.dump(whole_dataset.mapping_inverse_dict, f)"
      ],
      "metadata": {
        "id": "vjsghT3UjwSK"
      },
      "execution_count": 384,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.save(model.state_dict(), \"/content/drive/MyDrive/DNNs2/final_model_state_dict.pth\")"
      ],
      "metadata": {
        "id": "zWCbDjGBctDp"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "holdout_inputs, holdout_labels = holdout[::]\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # (just splitting up the data, otherwise it doesn't fit in the model)\n",
        "    probs = F.softmax(torch.cat([model(input_chunk) for input_chunk in torch.split(holdout_inputs, 50000)]), dim=-1)\n",
        "    predicted = probs.argmax(dim=-1)\n",
        "    accuracy = (predicted==holdout_labels).float().mean().item()\n",
        "    print('\\tAccuracy of \"best-guess\"es on the holdout set:\"', f\"{accuracy*100:.4f}%\", \"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONUd39AS4JW5",
        "outputId": "95fd943b-3f2d-4106-ccf1-90e7ef6b4cb5"
      },
      "execution_count": 393,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tAccuracy of \"best-guess\"es on the holdout set:\" 99.9800% \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unscheduledModel = EncodedSymmetricGroupTransformer(vocab_size=factorial(base_set_size), embed_dim=64, num_heads=4,\n",
        "                                                    num_layers=3, dropout=0.2).cuda()\n",
        "\n",
        "train_model(unscheduledModel, train, validation, num_epochs=1000, learning_rate=0.0035,\n",
        "            num_batches=1, weight_decay=0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJB9e8JloSr5",
        "outputId": "68b31373-2079-48b4-8bbf-26b135261547"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch, Training Loss, Validation Loss):\n",
            "   (0, 6.7037, 6.6955)   (200, 0.0247, 14.5634)   (400, 0.0124, 17.4340)   (600, 0.0008, 18.7432)   (800, 0.0000, 19.6541)   (1000, 0.0000, 20.4355)\n",
            "    Accuracy of \"best-guess\"es on the validation set:\" 0.08% \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unscheduledSmallModel = EncodedSymmetricGroupTransformer(vocab_size=factorial(base_set_size), embed_dim=16, num_heads=4,\n",
        "                                                       num_layers=3, dropout=0.5).cuda()\n",
        "\n",
        "train_model(unscheduledSmallModel, train, validation, num_epochs=1000, learning_rate=0.001,\n",
        "            num_batches=5, weight_decay=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aturo0rcrwtR",
        "outputId": "7c0d9413-5e9c-4c2d-d35f-077b4bbc31d8"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch, Training Loss, Validation Loss):\n",
            "   (0, 6.7126, 6.7279)   (200, 5.9495, 7.0703)   (400, 5.1723, 7.8606)   (600, 5.1170, 7.8337)   (800, 5.1302, 7.8067)   (1000, 5.1138, 7.7529)\n",
            "    Accuracy of \"best-guess\"es on the validation set:\" 0.20% \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOOOO! The comparison (which'll be quicker) should be with \"a randomly selected training sample of *the same length*.\n",
        "\n",
        "\n",
        "Going for \"way bigger dataset and still worse!!\" is cool, but it's so expensive to compute, and the former claim is still significant."
      ],
      "metadata": {
        "id": "7txn7r-Avtfs"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "koMibeX-o_Hm",
        "jBtDchsshdSL",
        "PgeBTAhxjiYN",
        "JUrAPcUtrWJj",
        "6uwv2662c5h4",
        "IThrnbTQiTmm",
        "huHgf5Da1M0w",
        "56f6sB0o5BDp",
        "P78M3FyP65FF",
        "HAefsoFPswD4"
      ],
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30918,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}