{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JDS289/DNNs/blob/main/finalC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#C: Inductive Bias of ReLU networks\n"
      ],
      "metadata": {
        "id": "pkwEStOO6BZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'd like to see if a probabalistic classifier will tend to learn generalisable probabbility reasoning, or whether this struggles to generalise beyond its training data. So for example, will a neural network end up implementing Bayes' Rule, or will it have a tendency towards just memorising its training distributions?"
      ],
      "metadata": {
        "id": "54txB2MYUVzD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I decided to make a simple \"Typo\" dataset: over a small list of words, suppose words are drawn from a distribution over this list, and then as each word is typed, each letter has a probability of being transmuted to another letter, based on the keyboard-distance of the letters. The important thing is not whether my toy dataset is realistic, but whether a neural network will learn these distributions and potentially some \"simplicity\" in its implementation, from inductive biases.   "
      ],
      "metadata": {
        "id": "J0uV3vCjL7kU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What I'm testing for, specifically:\n",
        "* *Generalised probabilistic reasoning:* Can it learn to reason \"even though this typo is closer to X, it's more likely to have been Y because Y is a much more common word\" (and vice versa) on an unseen typo?\n",
        "* *Smoothness / Irrelevance-Invariance:* Will its learned behaviour represent an elegant function i.e. will its output stay the same when modifying an irrelevant part of the input?"
      ],
      "metadata": {
        "id": "ccDjYSXkPUxJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Implementation"
      ],
      "metadata": {
        "id": "hLlFfzzzZKWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The inputs (all four-letter strings) were encoded as follows:\n",
        "we have a keyboard of 30 characters; flatten this to 1d and then use letter positions as a letter -> number encoding; now the input-layer of 120 neurons is such that neuron i is set to 1 if *letter i//30 of the input string is in keyboard-position i%30*, otherwise 0. The network outputs are put through a Softmax, and it's trained using log loss, with Adam.\n",
        "\n",
        "I experimented with the number of hidden layers, by testing its loss on a hold-out set. I chose 10, as it seemed to work fairly well and didn't overfit much (higher numbers of layers ended up being too slow for me to train)."
      ],
      "metadata": {
        "id": "zpARTGV8ZOuP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Results"
      ],
      "metadata": {
        "id": "RCvCY-IvXzGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keyboard = np.array([list(\"qwertyuiop\"), list(\"asdfghjkl;\"), list(\"zxcvbnm,./\")])\n",
        "flat_keyboard = keyboard.flatten()\n",
        "\n",
        "def prob_typo(char1, char2):  # unscaled logits\n",
        "    return 7 / (1+distance(char1, char2))\n",
        "\n",
        "def get_typo_distribution(intended_letter):\n",
        "    return sorted(zip(flat_keyboard, torch.sigmoid(normalize_logits(np.array([prob_typo(intended_letter, char) for char in flat_keyboard])))), key=lambda t: -t[1])\n",
        "\n",
        "def format_distribution(intended_letter, length=10):\n",
        "    return f\"Typo distribution from {intended_letter}:\\n\" + \"\\n\".join(map(lambda t: f\"{t[0]}: {round(t[1].item(), 3)}\", get_typo_distribution(intended_letter)[:length]))\n",
        "\n",
        "print(format_distribution(\"e\"))\n",
        "print(\"\\n\")\n",
        "print(format_distribution(\"a\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "qmyPZYptfIlG",
        "outputId": "7bc34e52-831d-4f72-ed9d-e3edacf5b31c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Typo distribution from e:\n",
            "e: 0.446\n",
            "w: 0.024\n",
            "r: 0.024\n",
            "d: 0.024\n",
            "s: 0.013\n",
            "f: 0.013\n",
            "q: 0.008\n",
            "t: 0.008\n",
            "c: 0.008\n",
            "a: 0.006\n",
            "\n",
            "\n",
            "Typo distribution from a:\n",
            "a: 0.453\n",
            "q: 0.024\n",
            "s: 0.024\n",
            "z: 0.024\n",
            "w: 0.014\n",
            "x: 0.014\n",
            "d: 0.008\n",
            "e: 0.007\n",
            "c: 0.007\n",
            "f: 0.004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(prediction(model, \"les[\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0e7cxZfnFt_",
        "outputId": "9fa6f9b1-52fb-4237-c750-0adca171b6ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "past: 0.21%\n",
            "last: 7.99%\n",
            "part: 0.02%\n",
            "pest: 5.62%\n",
            "lest: 86.16%\n",
            "kart: 0.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Single-quotes means what was intended; double-quotes means what was typed:\n",
        "\n",
        "$$\n",
        "\\text{Bayesianism using the actual distributions: }~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\\\ \\frac{P(`\\text{lest'} ~ | ~ ``\\text{les[\"})}{P(`\\text{last'} ~|~ ``\\text{les[\"})} = \\frac{P(``\\text{les[\"} ~ | ~ `\\text{lest'}) ~ P(`\\text{lest'}) / P(``\\text{les[\"})} {P(``\\text{les[\"} ~ | ~ `\\text{last'}) ~ P(`\\text{last'}) / P(``\\text{les[\"})} \\\\[0.3in] =\n",
        "0.2 ~ \\frac{{P(``\\text{les[\"} ~ | ~ `\\text{lest'}})} {{P(``\\text{les[\"} ~ | ~ `\\text{last'}})} = 0.2 ~ \\frac{P(``\\text{e\"} ~|~ `\\text{e'}) }{P(``\\text{e\"} ~|~ `\\text{a'}) }~\\approx~ 0.2 ~  \\frac{0.446}{0.007} ~~\\approx~ 12.7\n",
        "\\\\[0.5in]\n",
        "\\text{Model's result, having never seen this example: }~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\\\\n",
        "\\frac{86.16}{7.99} ~ \\approx ~ 10.8\n",
        "\\\\[0.6in]\n",
        "$$"
      ],
      "metadata": {
        "id": "5lM546uC0ZCI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is pretty close, and particularly impressive for an example it had never seen during training. In fact, something non-trivial that the model handled well: a typed \"r\" in second position is enough to overcome the prior of P('last') = 5 P('lest'), but a \"d\" in second position is not (that is, an \"r\" makes the posterior probability of 'lest' overtake 'last', whereas a \"d\" does not) -- this is because although 'e'→\"d\" is a 4× more likely typo than 'a'→\"d\", this is still less than the prior of 5×, whereas 'e'→\"r\" is **6**× more likely than 'a'→\"r\".\n",
        "\n",
        "\n",
        "So it seems like it has learned some generalisable notions about probability, rather than just memorising the training answers."
      ],
      "metadata": {
        "id": "Ummkw_Gn3h8f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another property we want -- a notion of \"invariance over irrelevant changes\", i.e. we don't want a change to the *final* letter to randomly change its preferences for \"past\" vs \"last\" (given that they only differ in the first letter, and my sample-generation assumed independence)."
      ],
      "metadata": {
        "id": "s6lvd7R-NnQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(prediction(model, \"pas.\"))\n",
        "print(\"\\n\")\n",
        "print(prediction(model, \"pas;\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5CZzHx9MRSz",
        "outputId": "2117ff26-6d7a-4561-90ad-2a08a80d7bc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "past: 96.59%\n",
            "last: 1.71%\n",
            "part: 1.01%\n",
            "pest: 0.68%\n",
            "lest: 0.0%\n",
            "kart: 0.0%\n",
            "\n",
            "\n",
            "past: 97.18%\n",
            "last: 1.8%\n",
            "part: 0.75%\n",
            "pest: 0.28%\n",
            "lest: 0.0%\n",
            "kart: 0.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is quite consistent -- the percentages are very similar, and the \"preference ranking\" stays identical. It's not perfect though, so the underlying function probably isn't as simple as we'd like -- but of course it could've been trained for much longer."
      ],
      "metadata": {
        "id": "iAecKbOGOkub"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "DeEax14-4ret",
        "pkwEStOO6BZH"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}